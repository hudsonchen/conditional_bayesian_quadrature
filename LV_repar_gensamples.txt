import stan
import numpy as np
import os

import csv
import pandas as pd
import torch
import matplotlib.pyplot as plt

# cwd = os.getcwd()
##### Lotka example
lotka_code = """
functions {
  real[] dz_dt(real t, // time
                     array[] real z,
                     // system state {prey, predator}
                     array[] real theta, // parameters
                     array[] real x_r, // unused data
                     array[] int x_i) {
    real u = z[1];
    real v = z[2];

    real alpha = theta[1];
    real beta = theta[2];
    real gamma = theta[3];
    real delta = theta[4];

    real du_dt = (exp(alpha) - exp(beta) * v) * u;
    real dv_dt = (-exp(gamma) + exp(delta) * u) * v;
    return {du_dt, dv_dt};
  }
}
data {
  int<lower = 0> N;           // num measurements
  real ts[N];                 // measurement times > 0
  real y_init[2];             // initial measured population
  real<lower = 0> y[N, 2];    // measured population at measurement times
  int<lower = 0> N_zgen;
  real ts_N_zgen[N_zgen]; 
}
parameters {
  real theta[4];   // theta = { alpha, beta, gamma, delta }
  real z_init[2];  // initial population
  real sigma[2];   // error scale
}
transformed parameters {
  real z[N, 2]
    = integrate_ode_rk45(dz_dt, exp(z_init), 0, ts, theta,
                         rep_array(0.0, 0), rep_array(0, 0),
                         1e-6, 1e-5, 1e3);
}
model {
  theta[{1, 3}] ~ normal(0, 0.5);
  theta[{2, 4}] ~ normal(-3, 0.5);
  sigma ~ normal(-1, 1);
  z_init ~ normal(log(10), 1);
  for (k in 1:2) {
    y_init[k] ~ lognormal(log(exp(z_init[k])), exp(sigma[k]));
    y[ , k] ~ lognormal(log(z[, k]), exp(sigma[k]));
  }
}
generated quantities {
  real z_gen[N_zgen, 2]
    = integrate_ode_rk45(dz_dt, exp(z_init), 0, ts_N_zgen, theta,
                         rep_array(0.0, 0), rep_array(0, 0),
                         1e-6, 1e-5, 1e3);
  array[2] real y_init_rep;
  array[N, 2] real y_rep;
  for (k in 1 : 2) {
    y_init_rep[k] = lognormal_rng(log(exp(z_init[k])), exp(sigma[k]));
    for (n in 1 : N) {
      y_rep[n, k] = lognormal_rng(log(z[n, k]), exp(sigma[k]));
    }
  }
}
"""

# Read inner integral data
lynx_hare_df = pd.read_csv("Lotka/hudson-bay-lynx-hare.csv", header=2)
N = len(lynx_hare_df) - 1
ts = np.arange(1, N + 1, 1).tolist()
y_init = [lynx_hare_df.iloc[0, 2], lynx_hare_df.iloc[0, 1]]  # c(lynx_hare_df$Hare[1], lynx_hare_df$Lynx[1])
y = np.array(lynx_hare_df.iloc[1:(N + 1), [2, 1]])  # hare, lynx order
y = y.tolist()

ts_N_zgen = np.arange(1, N + 1, 0.2).tolist()
N_zgen = len(ts_N_zgen)

lynx_hare_data = {'N': N, 'ts': ts, 'y_init': y_init, 'y': y, 'N_zgen': N_zgen, 'ts_N_zgen': ts_N_zgen}

#  Build model and sample
nchains = 8
nsams_perchain = 10000
# nsams_perchain = 200
posterior = stan.build(program_code=lotka_code, data=lynx_hare_data, random_seed=123)
fit = posterior.sample(num_chains=nchains, num_samples=nsams_perchain)



#
z_init = fit['z_init']
sigma = fit['sigma']
theta = fit["theta"]

z_init_tensor = torch.tensor(z_init)  # 2d tensor of size [2, num_chains*num_samples]
sigma_tensor = torch.tensor(sigma)  # 2d tensor of size [2, num_chains*num_samples]
theta_tensor = torch.tensor(theta)  # 2d tensor of size [4, num_chains*num_samples]


# theta_tensor.mean(1)

samples_from_posterior_pdframe = fit.to_frame()  # pandas `DataFrame, requires pandas
print(samples_from_posterior_pdframe.describe().T)

list(samples_from_posterior_pdframe.columns.values)
# Analyse the posteriors and see what can we do for



score_unconstrainedsamples = torch.zeros(nchains * nsams_perchain, 8)
for i in range(nchains * nsams_perchain):
    #  Zhuo: for me, after reading the booklet, I think,
    #         function 'unconstrain_pars' takes in constrained values and returns the corresponding unconstrained values.
    #         then the function 'grad_log_prob' will calculate the gradient of log posterior at the corresponing unconstrained values.
    temp = posterior.grad_log_prob(posterior.unconstrain_pars(dict(theta=theta[:, i].tolist(), \
                                                                   z_init=z_init[:, i].tolist(), \
                                                                   sigma=sigma[:, i].tolist())))
    score_unconstrainedsamples[i] = torch.tensor(temp)


# Posterior Predictive Samples of observations

y_rep_tensor.size()
# 3d tensor of size [N, 2, num_chains*num_samples], the last dim equals the number of posterior samples

y_init_rep_tensor = torch.tensor(fit['y_init_rep'])
y_init_rep_tensor.size()
# 2d tensor of size [2, num_chains*num_samples], the last dim equals the number of posterior samples





# Save data
X_all = torch.cat((theta_tensor, z_init_tensor, sigma_tensor), 0)
X_all = X_all.t().float()
X_all.size()

z_rep_tensor = torch.tensor(fit['z_gen'])
z_rep_tensor.float().size()



# Save data
# torch.save(X_all.float().detach().clone(), 'Lotkarepar_posteriorsamples.pt')
# torch.save(z_rep_tensor.float().detach().clone(), 'Lotkarepar_u_values.pt')
# torch.save(score_unconstrainedsamples.float().detach().clone(), 'Lotkarepar_score_posterior.pt')

# torch.save(y_rep_tensor.float().detach().clone(), 'Lotkarepar_yppc.pt')



## Visualization
if __name__ =="__main__":
    task_indices_list = np.arange(50, 60, 1).tolist()
    # task_indices_list = np.arange(60, 70, 1).tolist()
    # task_indices_list = np.arange(65, 70, 1).tolist() very good

    task_time_points = np.array(ts_N_zgen)[task_indices_list] + 1901

    mean_ppc = y_rep_tensor.mean(dim=2)
    CriL_ppc = torch.quantile(y_rep_tensor, q=0.025, dim=2)
    CriU_ppc = torch.quantile(y_rep_tensor, q=0.975, dim=2)

    Year = np.arange(1901, 1920 + 1, 1)
    plt.figure(figsize=(15, 2 * (5)))
    plt.subplot(2, 2, 1)
    plt.plot(Year, torch.tensor(y)[:, 1].numpy(), "o", color="b", lw=4, ms=10.5)  # 0 is hare, 1 is lynx
    plt.plot(Year, mean_ppc[:, 1].numpy(), color="b", lw=4)
    plt.plot(Year, CriL_ppc[:, 1].numpy(), "--", color="b", lw=2)
    plt.plot(Year, CriU_ppc[:, 1].numpy(), "--", color="b", lw=2)

    plt.xlim([1900, 1921])
    plt.ylim([0, 80])
    plt.xlabel("Year", fontsize=15)
    plt.ylabel("Lynx", fontsize=15)
    plt.xticks(Year, rotation=45)

    plt.subplot(2, 2, 2)
    plt.plot(Year, torch.tensor(y)[:, 0].numpy(), "o", color="g", lw=4, ms=10.5, label="Observed")
    plt.plot(Year, mean_ppc[:, 0].numpy(), color="g", lw=4, label="mean of ppc")
    plt.plot(Year, CriL_ppc[:, 0].numpy(), "--", color="g", lw=2, label="credible intervals")
    plt.plot(Year, CriU_ppc[:, 0].numpy(), "--", color="g", lw=2)
    plt.vlines(min(task_time_points), 0, 120, colors='r')
    plt.vlines(max(task_time_points), 0, 120, colors='r')
    plt.xlim([1900, 1921])
    plt.ylim([0, 120])
    plt.legend(fontsize=15)
    plt.xlabel("Year", fontsize=15)
    plt.ylabel("Hare", fontsize=15)
    plt.xticks(Year, rotation=45)
    # plt.show()

    ### ------- z_gen ---
    # fit.constrained_param_names
    z_rep_tensor = torch.tensor(fit['z_gen'])
    # 3d tensor of size [N, 2, num_chains*num_samples], the last dim equals the number of posterior samples

    # plt.figure(figsize=(15, 2 * (5)))
    Year = np.arange(1901, 1920 + 1, 0.2)
    mean_ppc = z_rep_tensor.mean(dim=2)
    CriL_ppc = torch.quantile(z_rep_tensor, q=0.025, dim=2)
    CriU_ppc = torch.quantile(z_rep_tensor, q=0.975, dim=2)

    plt.subplot(2, 2, 3)
    # plt.plot(Year, torch.tensor(y)[:, 1].exp().numpy(), "o", color="b", lw=4, ms=10.5)  # 0 is hare, 1 is lynx
    plt.plot(Year, mean_ppc[:, 1].numpy(), color="b", lw=4)
    plt.plot(Year, CriL_ppc[:, 1].numpy(), "--", color="b", lw=2)
    plt.plot(Year, CriU_ppc[:, 1].numpy(), "--", color="b", lw=2)

    plt.xlim([1900, 1921])
    plt.ylim([0, 80])
    plt.xlabel("Year", fontsize=15)
    plt.ylabel("$u_2$", fontsize=15)
    plt.xticks(range(int(min(Year.tolist())), int(max(Year)+1)), rotation=45)


    plt.subplot(2, 2, 4)
    # plt.plot(Year, torch.tensor(y)[:, 0].exp().numpy(), "o", color="g", lw=4, ms=10.5, label="Observed")
    plt.plot(Year, mean_ppc[:, 0].numpy(), color="g", lw=4, label="mean of u")
    plt.plot(Year, CriL_ppc[:, 0].numpy(), "--", color="g", lw=2, label="credible intervals")
    plt.plot(Year, CriU_ppc[:, 0].numpy(), "--", color="g", lw=2)
    plt.vlines(min(task_time_points), 0, 120, colors='r')
    plt.vlines(max(task_time_points), 0, 120, colors='r')

    plt.xlim([1900, 1921])
    plt.ylim([0, 120])
    plt.legend(fontsize=15)
    plt.xlabel("Year", fontsize=15)
    plt.ylabel("$u_1$", fontsize=15)
    plt.xticks(range(int(min(Year.tolist())), int(max(Year)+1)), rotation=45)
    plt.show()

    # plt.savefig("Lotka.pdf")
